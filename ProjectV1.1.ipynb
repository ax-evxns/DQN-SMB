{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import os\n",
    "import types\n",
    "import gym_super_mario_bros\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from collections import deque\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# To properly run/create your own model you need gym version 0.23.0\n",
    "# and need to install Microsoft C++ Build Tools with the Windows 10 SDK & C++ x64/x86 build tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784cc6a",
   "metadata": {},
   "source": [
    "Custom Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(info, max_x, t, flag):\n",
    "    # Easily the most important/critical part of the entire file. Any broken or not fully thought out code\n",
    "    # will result in a failed model. This right here took the longest to get right. The agent will ALWAYS\n",
    "    # exploit any weaknesses in the code. This will cause enexpected behavior and complete loss of model\n",
    "    # Default reward function does not penelize stalling or reward getting to flagpole. It also uses\n",
    "    # a current x pos minus previous x pos reward which allows the agent to shuffle back and forth to gain\n",
    "    # rewards and not make progress. While not clipped the max reward per frame is >10 and max penelty >-15\n",
    "\n",
    "    x_pos, y_pos, flag_get = info['x_pos'], info['y_pos'], info['flag_get'] # grab some current values\n",
    "    delta = x_pos - max_x # calculate total progress made based off pixels traveled in a frame\n",
    "    ch = 0 # checkpoint reward\n",
    "    v = 0 # v(elocity) reward\n",
    "    y = 0 # y_position penelty/reward\n",
    "    done = False # end episode when stalled to long\n",
    "    reward = 0.0 \n",
    "\n",
    "    if delta > 1: # made progress, over 1 pixels (prevents mini progress on pipes but still stuck)\n",
    "        v = delta\n",
    "        max_x = x_pos\n",
    "        t = 0\n",
    "\n",
    "    # elif x_pos > 2870 and max_x >= 3000: # transition levels break progress reward, this mitigates that\n",
    "    #     max_x = x_pos\n",
    "    #     v = x_pos - max_x\n",
    "    #     t = 0\n",
    "\n",
    "    else: # not making progress, start counter\n",
    "        t += 1\n",
    "        if t > 60: # if after 60 frames the agent has not made progress, begin penelties\n",
    "            \n",
    "            y = -2 + (2.85 * ((y_pos - 79) / 240)) # to prevent the agent from essentially giving up when getting\n",
    "                                                   # stuck, we give a mini reward for jumping. Without this, because\n",
    "                                                   # the penelty would be the same regardless of what action is taken\n",
    "                                                   # the qvalues will be very similar for all actions\n",
    "                                                   # (they would all give the same reward/penelty) this is because\n",
    "                                                   # the only real way the agent gains rewards is based off x_pos\n",
    "                                                   # progress. So to make sure the model knows there are better\n",
    "                                                   # actions, we reward jumps when stalled with the higher jump\n",
    "                                                   # the better reward\n",
    "\n",
    "            y = min(max(y, -2), -0.1) # Still, the model is stuck and not progressing so jumping only lowers\n",
    "                                      # penelties, not eliminate them. This is enough info to make the model jump\n",
    "\n",
    "        if t > 500: # if after 500 frames (about 10 seconds?) end episode and apply death penelty\n",
    "            done = True\n",
    "\n",
    "    c = -0.1 # time penelty\n",
    "\n",
    "    if x_pos > 1320 and not flag: # reached checkpoint give reward once\n",
    "        ch = 10\n",
    "        flag = True\n",
    "\n",
    "    reward = v + ch + (y / 4) + c # all reward, since this is per frame and the rewards given to model is for 4 frames\n",
    "                                  # we lower the stall penelty so the agent doesnt get overwhelmed\n",
    "\n",
    "    if flag_get:\n",
    "        reward += 25  # large reward for completing the level\n",
    "        done = True\n",
    "        print(\"WE DID IT\") # Once the model consistantly reaches this, its essentially done\n",
    "\n",
    "    #print(f\"v: {v:.3f}, y: {y:.3f}, c: {c:.3f}, t: {t}, x: {x_pos:.3f}\") # metrics\n",
    "    return reward, max_x, t, flag, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5e74a-4af5-45e5-a1eb-17700cc94d7c",
   "metadata": {},
   "source": [
    "Setup Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStackAndSkip(gym.Wrapper):\n",
    "    # Modifies the original enviroment function to: Implement frameskip, stack the skipped frames as 1 NN\n",
    "    # input and modifies reward and death function. Orignal death function skipped death frames leading to\n",
    "    # insufficient learning on the NN as it didn't know what death looks like.\n",
    "    # For this we need to grab certain ram values and obtain the raw enviroment with no functions applied.\n",
    "\n",
    "    def __init__(self, env, skip=4, target_size=(96, 96)): # As neural network input we use a 4x96x96 image array\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "        self.frames = deque(maxlen=skip)\n",
    "        raw = env\n",
    "        while hasattr(raw, 'env'):\n",
    "            raw = raw.env\n",
    "        \n",
    "        # the env.get_done function has a frame advance line in it essentially skipping the frame where the\n",
    "        # agent dies. This makes the model skip what caused a death and a death penelty. We intercept this\n",
    "        # function to use our own and not advance the frame.\n",
    "\n",
    "        # keep a reference to the original _get_done\n",
    "        raw._orig_get_done = raw._get_done\n",
    "\n",
    "        # override _get_done to end episode on dying state\n",
    "        def _get_done_override(self):\n",
    "            # if Mario is starting to die or already dead, signal done\n",
    "            if self._is_dying or self._is_dead:\n",
    "                return True\n",
    "            # otherwise fall back to original logic\n",
    "            return self._orig_get_done()\n",
    "\n",
    "        raw._get_done = types.MethodType(_get_done_override, raw)\n",
    "        self.in_death = False\n",
    "        self.transform = T.Compose([ # Preprocessing images: converts to tensor and resizes\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(target_size),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.in_death = False\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        processed = self.transform(obs)\n",
    "        for _ in range(self._skip):\n",
    "            self.frames.append(processed)\n",
    "        return torch.cat(list(self.frames), dim=0)  # Stack along the channel dimension\n",
    "    \n",
    "    def step(self, action, max_x, t, flag, died_flag):\n",
    "        # Per step aka action per frame, we need some unaccessable items like y_viewport (which is the y position\n",
    "        # of mario) and player_state (what the agents state is which includes whether they died or not)\n",
    "        # We also apply our custom reward function here per frame as to not lose temporal data\n",
    "        # This is where death is seen and penelties applied we also apply a reward if agent has beat level\n",
    "\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for _ in range(self._skip):\n",
    "            if done:\n",
    "                break\n",
    "            obs, _, done, info = self.env.step(action)\n",
    "            info['player_state'] = self.env.unwrapped.ram[0x000e]  # Access RAM directly, 11 & 6 means dead/dying\n",
    "            info['y_viewport'] = self.env.unwrapped.ram[0x00b5] # anything >1 means falling in pit aka dead\n",
    "            if (info['player_state'] == 11 or info['player_state'] == 6 or info['y_viewport'] > 1) and not died_flag: # is dying or died\n",
    "                total_reward = -10 # died\n",
    "                died_flag = True\n",
    "                processed = self.transform(obs)\n",
    "                self.frames.append(processed)\n",
    "                break\n",
    "\n",
    "            if died_flag:\n",
    "                processed = self.transform(obs)\n",
    "                self.frames.append(processed)\n",
    "                break\n",
    "\n",
    "            reward, max_x, t, flag, done = custom_reward(info, max_x, t, flag)\n",
    "            total_reward += reward\n",
    "            processed = self.transform(obs)\n",
    "            self.frames.append(processed) # combines the frameskipped frames then concatenate them on return\n",
    "            if done: # sometimes if the agent wins or dies in the middle of a skipped frame it might not get caught\n",
    "                if info['flag_get']: # so we call this again to really make sure\n",
    "                    total_reward = 25 # won\n",
    "                else:\n",
    "                    total_reward = -10\n",
    "                break\n",
    "        return torch.cat(list(self.frames), dim=0), total_reward, done, info, max_x, t, flag, died_flag\n",
    "\n",
    "# Initialize the game environment, initialize tensorboard for metrics and convert images given by original\n",
    "# step method to grayscale as color doesn't really matter in this game, we also apply the 'controller'\n",
    "# the agent will use called \"SIMPLE_MOVEMENT\" which allows multiple button presses. Exact actions are in training\n",
    "# loop. Then we apply our custom step method and the enviroment initialization is complete.\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "writer = SummaryWriter(log_dir=os.path.join(current_dir, \"runs\"))\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-2-2-v1') \n",
    "env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = FrameStackAndSkip(env)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3921141-5870-42ba-9b6f-048347cac9a2",
   "metadata": {},
   "source": [
    "Define Parameters/Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37d12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyper parameters\n",
    "\n",
    "alpha = 2e-4 # learning rate 1e-4 & 2e-4 were used depending on how many episodes were needed\n",
    "epsilon = 0.99 # best action vs random action, begin with only random then lower as agent learns to play\n",
    "decay = 0.999985 # epsilon is decayed per step* (not per episode) this is for custom epsilon reset in training\n",
    "gamma = 0.95 # prioritize future rewards\n",
    "replay_buffer2 = 75000 # the size of experience replay buffer\n",
    "batch_size = 128 # samples grabbed from experience replay\n",
    "targ_updater = 125 # update target nn every 125 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304aaf9",
   "metadata": {},
   "source": [
    "Implement Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastReplayBuffer:\n",
    "    # Experience replay breaks temporal correlation bias and allows the model to learn each frame\n",
    "    # independantly from the others resulting in a more robust and generalizable model. We only need\n",
    "    # a few functions which is push to add a state after an action and sample aka push when grabbing\n",
    "    # some RANDOM states for the model training.\n",
    "\n",
    "    def __init__(self, capacity, state_shape, device):\n",
    "        self.capacity = capacity\n",
    "        self.device   = device\n",
    "        self.states      = torch.zeros((capacity, *state_shape), dtype=torch.float32, device=device) # Pre‑allocate tensors on GPU\n",
    "        self.next_states = torch.zeros_like(self.states, device=device)\n",
    "        self.actions     = torch.zeros((capacity,), dtype=torch.int64,   device=device)\n",
    "        self.rewards     = torch.zeros((capacity,), dtype=torch.float32, device=device)\n",
    "        self.dones       = torch.zeros((capacity,), dtype=torch.bool,    device=device)\n",
    "        self.pos  = 0\n",
    "        self.full = False\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        idx = self.pos\n",
    "        self.states[idx]      = state\n",
    "        self.next_states[idx] = next_state\n",
    "        self.actions[idx]     = action\n",
    "        self.rewards[idx]     = reward\n",
    "        self.dones[idx]       = done\n",
    "        self.pos = (idx + 1) % self.capacity\n",
    "        if self.pos == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_idx = self.capacity if self.full else self.pos\n",
    "        idx = torch.randint(0, max_idx, (batch_size,), device=self.device) # sample indices directly on GPU\n",
    "        return (\n",
    "            self.states[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.next_states[idx],\n",
    "            self.dones[idx],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c6559",
   "metadata": {},
   "source": [
    "Create Main & Target Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d95380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    # Some NN were tried like MLP, CNNs and now for the final iteration, a dueling DQN with CNNs.\n",
    "    # Dueling DQNs are split into two heads: value and advantage. The value head calculates how good of a\n",
    "    # state it is currently in. Obviously some states are better than others like falling down a pit (bad) vs\n",
    "    # close to the flagpole with velocity (great). The advantage head then calculates the reward or penelty\n",
    "    # of each action. For example, if the agent is over a pit, during those frames the advantage head will \n",
    "    # calculate the actions right+run+jump and left (along with all others but we will use these for the example)\n",
    "    # the model will learn that falling into pit = very bad so the 'left' action will probably have a negative\n",
    "    # value during these frames while the other will have a high positive value, leading the model to choose this.\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        # Since we are using images as the input, we need to perform some convolutions so this is the cnn\n",
    "        # section of the model\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.flatten_size = self._get_conv_output((4, 96, 96))\n",
    "\n",
    "        # --- Advantage Head ---\n",
    "        self.adv_fc1 = nn.Linear(self.flatten_size, 256)\n",
    "        self.adv_fc2 = nn.Linear(256, output_size)\n",
    "\n",
    "        # --- Value Head ---\n",
    "        self.val_fc1 = nn.Linear(self.flatten_size, 256)\n",
    "        self.val_fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        # When evaluating we use a batch size of 1, when using experience replay we use batchsize 128\n",
    "        # this will determine the input shape to the model\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, *shape)\n",
    "            x = self.bn1(self.conv1(x))\n",
    "            x = self.bn2(self.conv2(x))\n",
    "            x = self.bn3(self.conv3(x))\n",
    "            return int(x.flatten(1).shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolute, normalize, ReLU then flatten and calculate value and advantage finally return the function below\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        # flatten\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # advantage branch\n",
    "        adv = F.relu(self.adv_fc1(x))\n",
    "        adv = self.adv_fc2(adv)  # shape [B, num_actions]\n",
    "\n",
    "        # value branch\n",
    "        val = F.relu(self.val_fc1(x))\n",
    "        val = self.val_fc2(val)  # shape [B, 1]\n",
    "\n",
    "        # combine into Q-values\n",
    "        # Q(s,a) = V(s) + (A(s,a) - Σ_a A(s,a))\n",
    "        # The action with the highest value on the second part of the equation will usually get picked\n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "\n",
    "# Instantiate your networks exactly as before:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "main_dqn = DuelingDQN(len(SIMPLE_MOVEMENT)).to(device)\n",
    "target_dqn = DuelingDQN(len(SIMPLE_MOVEMENT)).to(device)\n",
    "target_dqn.load_state_dict(main_dqn.state_dict()) # target is just a clone of main or \"online\" model\n",
    "\n",
    "optimizer = torch.optim.Adam(main_dqn.parameters(), lr=alpha)\n",
    "criterion = nn.MSELoss()\n",
    "memory = FastReplayBuffer(\n",
    "    capacity=replay_buffer2,\n",
    "    state_shape=(4, 96, 96),\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "# Used if want to continue training/load model/ test model\n",
    "\n",
    "# main_dqn.load_state_dict(torch.load(os.path.join(current_dir, 'main_dqn_1-1.pt')))\n",
    "# target_dqn.load_state_dict(torch.load(os.path.join(current_dir, 'targ_dqn_1-1.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a953c4e",
   "metadata": {},
   "source": [
    "Develop Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72077f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where all training occurs. Many, many iterations of this were made but this is what is currently\n",
    "# the best. Each line will be commented for its importance\n",
    "\n",
    "num_episodes = 4000 # Number of episodes to train (varies based on level complexity)\n",
    "replay_active = False # exp. replay doesnt activate till it has enough samples, this ensures metrics dont break\n",
    "global_steps = 0 # metric to monitor number of total steps across all episode\n",
    "\n",
    "global_max_x = 40 # Sometimes levels are so complex, the agent only learns part of the level before epsilon\n",
    "                  # reaches its minimum. This keeps the maximum x position the agent has made across ALL episodes.\n",
    "\n",
    "max_x_dq = deque(maxlen=25) # some levels have trasitions that change the agents x position instantly and can\n",
    "                            # cause large penelties based on reward logic\n",
    "\n",
    "counter = 0 # used for training logic above\n",
    "\n",
    "try:\n",
    "    for episode in range(num_episodes):\n",
    "        # initialize certain variables per episode\n",
    "        max_x, t, flag, died_flag = 40, 0, False, False # for reward logic\n",
    "        done, episode_reward, step_count = False, 0, 0\n",
    "        o_flag = False \n",
    "        obs = env.reset() # reset environment back to beginning\n",
    "        state = obs.clone().detach().unsqueeze(0) # add a dimension for batch size (1, 4, 96, 96)\n",
    "        # prev_stage, prev_world = 1, 1\n",
    "\n",
    "        while not done: # per step (frame) loop\n",
    "            env.render() # shows the game\n",
    "            global_steps += 1\n",
    "  \n",
    "            if 0.1 >= 0.99 * (decay **global_steps):\n",
    "                # Even when the model has learned the entire level, some randomness is good\n",
    "                # the minimum episilon is 0.1 always during training and when enough steps have passed.\n",
    "                # However if the model has reached the minimum and has not seen all the level yet\n",
    "                # the model will not be able to choose the best action in that state since it hasnt been\n",
    "                # determined. When this happens its best to up epsilon back up until the model has sufficiently\n",
    "                # learned that area.\n",
    "\n",
    "                if (info['x_pos'] > global_max_x) and not o_flag: # if in a never before seen area...\n",
    "                    epsilon = 0.5 # raise epsilon\n",
    "                    decay = 0.998 # now uses much more aggressive per step decay since it will only last for 1 episode\n",
    "                    o_flag = True # mark this flag true so it doesnt keep setting epsilon to 0.5\n",
    "                else:\n",
    "                    epsilon = 0.1\n",
    "       \n",
    "            if np.random.rand() < epsilon: # Choose a random action (explore)\n",
    "                action = np.random.randint(0, len(SIMPLE_MOVEMENT))\n",
    "            else:\n",
    "                with torch.no_grad(): # Choose action based on model (exploit)\n",
    "                    q_values = main_dqn(state.to(device))\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            epsilon = max(0.1, epsilon * decay) # decay epsilon per step\n",
    "\n",
    "            # Step in the environment, grabs rewards, next state, info, and whether the episode is done or not\n",
    "            # the others are for reward tracking\n",
    "            next_obs, reward, done, info, max_x, t, flag, died_flag = env.step(action, max_x, t, flag, died_flag)\n",
    "\n",
    "            # if (info['stage'] > prev_stage) or (info['world'] > prev_world): # used for multi-level models\n",
    "            #     max_x = 40\n",
    "\n",
    "            next_state = next_obs.clone().detach().unsqueeze(0)  # Add extra dimension (1, 4, 96, 96)\n",
    "\n",
    "            memory.push(state, action, reward, next_state, done) # Store experience in replay buffer\n",
    "\n",
    "            state = next_state # We've reached the end of the step loop, update future state to current\n",
    "            episode_reward += reward\n",
    "\n",
    "            # prev_stage = info['stage'] # Used for multi-level models\n",
    "            # prev_world = info['world']\n",
    "\n",
    "            if len(memory) > batch_size: # Begin training with experience replay\n",
    "                replay_active = True\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size) # Grab (batchsize) 128 samples\n",
    "\n",
    "                with torch.no_grad(): # Calculate qvalue with target network and use bellman equation\n",
    "                    next_q_values = target_dqn(next_states).max(dim=1)[0]\n",
    "                    targets = rewards + gamma * next_q_values * (~dones)\n",
    "                \n",
    "                q_values = main_dqn(states).gather(1, actions.unsqueeze(1)).squeeze(1) # Resize output for loss calcs\n",
    "\n",
    "                loss = criterion(q_values, targets) # MSE loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step() # Optimize model, backpropagate, etc\n",
    "\n",
    "        if episode % targ_updater == 0: # Periodically update target network\n",
    "            target_dqn.load_state_dict(main_dqn.state_dict())\n",
    "\n",
    "        if global_max_x < max_x: # Some levels have transitions that break the reward function, this mitigates/fixes\n",
    "                                 # because of the random actions, its theoretically possible the agent can beat the \n",
    "                                 # level randomly, we use a counter the agent doesnt randomly reach a max_x luckily\n",
    "                                 # then we choose the minimum based off 25 maximum x positions taken from 25 episodes\n",
    "            counter += 1\n",
    "            max_x_dq.append(max_x)\n",
    "            if counter == 25:\n",
    "                if global_max_x >= min(max_x_dq):\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    global_max_x = min(max_x_dq)\n",
    "                    counter = 0\n",
    "\n",
    "        if replay_active: # write to tensorboard logs\n",
    "            writer.add_scalar(\"Reward\", episode_reward, episode)\n",
    "            writer.add_scalar(\"Loss\", loss.item(), episode)\n",
    "            writer.add_scalar(\"Epsilon\", epsilon, episode)\n",
    "            writer.add_scalar(\"GMax_X\", global_max_x, episode)\n",
    "            writer.add_scalar(\"Steps\", global_steps, episode)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted.\")\n",
    "\n",
    "writer.close()\n",
    "# actions are [nothing, right, right+jump, right+run, right+run+jump, jump, left]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36fdfa",
   "metadata": {},
   "source": [
    "Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bdff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model is probably the easiest part, first we lower epsilon to 0.05 as we now want\n",
    "# the majority of actions to be made by the model, however the model actually expects some random\n",
    "# actions since thats how it was trained. Lowering it to 0 will break the model unless it is overfit for\n",
    "# that specific level, then we lower the speed of the envitonment so that it is watchable. The\n",
    "# function is otherwise the same as training without the actual training part, experience replay is also not used\n",
    "# or needed here. If you just want to test the model, skip training and use this\n",
    "\n",
    "num_episodes = 200 \n",
    "epsilon = 0.05 # reduce epsilon to allow model to make most decisions\n",
    "\n",
    "try:\n",
    "    for episode in range(num_episodes):\n",
    "        max_x, t, flag, died_flag = 40, 0, False, False\n",
    "        #prev_stage, prev_world = 1, 1\n",
    "        stime = time.time()\n",
    "        obs = env.reset()\n",
    "        state = obs.clone().detach().unsqueeze(0)\n",
    "\n",
    "        done, episode_reward, step_count = False, 0, 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            step_count += 1\n",
    "            time.sleep(1/18) # slow down rendering to about normal\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(0, len(SIMPLE_MOVEMENT))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = main_dqn(state.to(device))  \n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_obs, reward, done, info, max_x, t, flag, died_flag = env.step(action, max_x, t, flag, died_flag)\n",
    "\n",
    "            # if (info['stage'] > prev_stage) or (info['world'] > prev_world):\n",
    "            #     max_x = 40\n",
    "\n",
    "            next_state = next_obs.clone().detach().unsqueeze(0)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            # prev_stage = info['stage']\n",
    "            # prev_world = info['world']\n",
    "            \n",
    "        etime = time.time()\n",
    "        print(f\"Episode: {episode} Reward: {episode_reward:.2f} Time: {(etime - stime):.2f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nEval interrupted.\")\n",
    "\n",
    "# action is [nothing, right, right+jump, right+run, right+run+jump, jump, left]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(main_dqn.state_dict(), \"main_dqn_-.pt\") # save our models to a file\n",
    "#torch.save(target_dqn.state_dict(), \"targ_dqn_-.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
